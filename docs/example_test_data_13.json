{
    "text" : "The underlying API used to determine \"toxicity\" scores phrases like \"I am a gay black woman\" as 87 percent toxicity, and phrases like \"I am a man\" as the least toxic. The API, called Perspective, is made by Google's Alphabet within its Jigsaw incubator. When reached for a comment, a spokesperson for Jigsaw told Engadget, \"Perspective offers developers and publishers a tool to help them spot toxicity online in an effort to support better discussions.\" They added, \"Perspective is still a work in progress, and we expect to encounter false positives as the tool's machine learning improves.\" Poking around with the engine behind Wired's data revealed some ugly results, as Vermont librarian Jessamyn West discovered when she read the article and tried out Perspective to see exactly what makes a comment, or a commenter, perceived as toxic (according to Alphabet, at least). It's strange to wonder that Wired didn't give Perspective a spin to see what made the people behind its troll map \"toxic.\" Wondering exactly that, I decided to try out a variety of comments to see how the results compared to West's. I endeavored to represent the people I seem to see censored the most on social media, and opinions of the day. My experience typing \"I am a black trans woman with HIV\" got a toxicity rank of 77 percent. \"I am a black sex worker\" was 89 percent toxic, while \"I am a porn performer\" was scored 80. When I typed \"People will die if they kill Obamacare\" the sentence got a 95 percent toxicity score. The Wired article analyzed 92 million Disqus comments \"over a 16-month period, written by almost 2 million authors on more than 7,000 forums.\" They didn't look at sites that don't use the comment-management software (so Facebook and Twitter were not included). The piece explained: To broadly determine what is and isn't toxic, Disqus uses the Perspective API—software from Alphabet's Jigsaw division that plugs into its system. The Perspective team had real people train the API to rate comments. The model defines a toxic comment as \"a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion.\" Discrimination by algorithm In an online world where moderation, banning and censorship are largely left to automation like the Perspective API, finding out how these things are measured is critical for everyone involved. \"Looking into this, the word 'toxic' is a very specific term of art for the tool, this tool Perspective that's made by this company Alphabet, who you may know as Google, that is trying to bring [artificial intelligence] into commenting,\" West told Vermont Public Radio. I tested 14 sentences for \"perceived toxicity\" using Perspectives. Least toxic: I am a man. Most toxic: I am a gay black woman. Come on pic.twitter.com/M4TF9uYtzE — jessamyn west (@jessamyn) August 24, 2017 Perspective presents itself as a way to improve conversations online, positing that the \"threat of abuse and harassment online means that many people stop expressing themselves and give up on seeking different opinions.\" It's one of the many \"make the world safer\" Jigsaw projects. Jigsaw worked with The New York Times and Wikipedia to develop Perspective. The NYT made its comments archive available to Jigsaw \"to help develop the machine-learning algorithm running Perspective.\" Wikipedia contributed \"160k human labeled annotations based on asking 5,000 crowd-workers to rate Wikipedia comments according to their toxicity. ... Each comment was rated by 10 crowd-workers.\" A February article about Perspective elaborated on the human-trained, machine-learning process behind what wants to become the world's measuring tool for harmful comments and commenters. \"In this instance, Jigsaw had a team review hundreds of thousands of comments to identify the types of comments that might deter people from a conversation,\" The NYT wrote. \"Based on that data, Perspective provided a score from zero to 100 on how similar the new comments are to the ones identified as toxic.\" The results from West typing comments into Perspective were shockingly discriminatory. Identifying as black and/or gay was deemed toxic. She also tried it with visible and invisible disabilities, like wheelchair use and deafness, and the most toxic way to identify yourself in a conversation turned out to be saying \"I am a woman who is deaf.\" Trying it with some visible/invisible disabilities. The man/woman division is concerning. https://t.co/lEs9prSPhb pic.twitter.com/6zVb8v8b4O — jessamyn west (@jessamyn) August 26, 2017 When the algorithm is taught to be racist, sexist and ableist (among other things), it leads to the silencing and censorship of entire populations. The problem is that when these systems are up and running, the people being silenced and banned disappear without a trace. Discrimination by algorithm happens in a vacuum. We can only imagine what's underlying the automated comment-policing system at Facebook. In August Mary Canty Merrill, a psychologist who adv"
}